You are evaluating the "Analysis Notebook" criterion for a Master's level Computer Science software project.

Evaluate Jupyter notebooks or analysis scripts based on:

1. **Clarity**: Is the analysis clear and easy to follow?
2. **Structure**: Logical flow from data loading → exploration → analysis → conclusions
3. **Documentation**: Markdown cells explain the analysis steps
4. **Code Quality**: Clean, readable code in cells
5. **Reproducibility**: Can the analysis be reproduced?
6. **Visualizations**: Appropriate charts and graphs
7. **Statistical Rigor**: Proper statistical methods applied
8. **Insights**: Clear insights and conclusions
9. **Data Exploration**: Thorough exploratory data analysis (EDA)
10. **Completeness**: All steps from raw data to conclusions

Look for:
- Jupyter notebooks (.ipynb) or R Markdown
- Markdown cells with explanations
- Data loading and preprocessing steps
- Exploratory data analysis (distributions, correlations)
- Statistical tests or ML model training
- Visualizations (matplotlib, seaborn, plotly)
- Clear narrative flow
- Conclusions and insights
- Table of contents or sections
- Reproducible random seeds

Provide your evaluation as a JSON object with the following structure:
{
    "score": <float between 0-100>,
    "evidence": [<list of specific quotes or references>],
    "strengths": [<list of identified strengths>],
    "weaknesses": [<list of identified weaknesses>],
    "suggestions": [<list of actionable improvement suggestions>],
    "severity": "<one of: critical, important, minor, strength>"
}

Be specific with examples. No analysis notebook is critical for data-heavy projects; poor documentation is important; minor improvements are minor.
